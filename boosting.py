# -*- coding: utf-8 -*-
"""boosting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yR8631T79mLnR4IPS9ELst_gblPuYGvx
"""

from __future__ import annotations
import matplotlib.pyplot as plt
from collections import defaultdict

import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.tree import DecisionTreeRegressor

from typing import Optional
from typing import Tuple


def score(clf, x, y):
    return roc_auc_score(y, clf.predict_proba(x)[:, 1])

# здесь нет GOSSа, делал его в посл очередь, поэтому класс с ним только в колабе
class Boosting:

    def __init__(
        self,
        base_model_class = DecisionTreeRegressor,
        base_model_params: Optional[dict] = None,
        n_estimators: int = 100,
        learning_rate: float = 0.1,
        early_stopping_rounds: int | None = 10,
        eval_set: Tuple[np.ndarray, np.ndarray] | None = None,
        subsample: float | int = 0.6,
        bagging_temperature: float | int = 1.0,
        bootstrap_type: str | None = 'Bernoulli'
    ):
        self.base_model_class = base_model_class
        self.base_model_params: dict = {} if base_model_params is None else base_model_params

        self.n_estimators: int = n_estimators

        self.models: list = []
        self.gammas: list = []

        self.learning_rate: float = learning_rate

        self.early_stopping_rounds: int = early_stopping_rounds
        self.eval_set: Tuple[np.ndarray, np.ndarray] | None = eval_set

        self.subsample: float | int = subsample
        self.bagging_temperature: float | int = bagging_temperature
        self.bootstrap_type: str | None = bootstrap_type


        self.history = defaultdict(list) # {"train_roc_auc": [], "train_loss": [], ...}

        self.sigmoid = lambda x: 1 / (1 + np.exp(-x))
        self.loss_fn = lambda y, z: -np.log(self.sigmoid(y * z)).mean()
        self.loss_derivative = lambda y, z: - y / (1 + np.exp(y * z)) # Исправьте формулу на правильную.


    def partial_fit(self, X, y):
        model = self.base_model_class(**self.base_model_params)
        if self.bootstrap_type is None:
          model.fit(X, y)
        elif self.bootstrap_type == 'Bernoulli':
          if isinstance(self.subsample, int):
            indexes_rand = np.random.choice(X.shape[0], size=self.subsample, replace=False)
          elif isinstance(self.subsample, float):
            indexes_rand = np.random.choice(X.shape[0], size=int(self.subsample * X.shape[0]), replace=False)
          model.fit(X[indexes_rand], y[indexes_rand])
        elif self.bootstrap_type == 'Bayesian':
          weights = (-np.log(np.random.uniform(0, 1, X.shape[0]))) ** self.bagging_temperature
          # будем проверять, не типа ли sparse_matrix у нас X
          if hasattr(X, "multiply"):
            X = X.multiply(weights[:, np.newaxis]) # weights[:, np.newaxis] переделывает shape w в (X.shape[0], 1)
          else:
            X = X * weights[:, np.newaxis]
          model.fit(X, y * weights)
        self.models.append(model)
        return model

    def fit(self, X_train, y_train, X_val=None, y_val=None, plot=False):
        """
        :param X_train: features array (train set)
        :param y_train: targets array (train set)
        :param X_val: features array (eval set)
        :param y_val: targets array (eval set)
        :param plot: bool
        """
        train_predictions = np.zeros(y_train.shape[0])

        if self.eval_set is not None:
          X_val = self.eval_set[0]
          y_val = self.eval_set[1]
          valid_predictions = np.zeros(y_val.shape[0])
          abs_of_imp_count = 0 # считаю, что early_stopping_rounds всегда больше 0 (как минимум 1)
          best_qual = -1


        for i in range(self.n_estimators):
            model = self.partial_fit(X_train, - self.loss_derivative(y_train, train_predictions))
            new_preds = model.predict(X_train)
            gamma = self.find_optimal_gamma(y_train, train_predictions, new_preds)
            self.gammas.append(gamma)
            train_predictions = train_predictions + gamma * self.learning_rate * new_preds
            self.history["train_loss"].append(self.loss_fn(y_train, train_predictions))
            self.history["train_roc_auc"].append(self.score(X_train, y_train))

            if self.eval_set is not None:
              new_valid_preds = model.predict(X_val)
              valid_predictions = valid_predictions + gamma * self.learning_rate * new_valid_preds
              self.history["valid_loss"].append(self.loss_fn(y_val, valid_predictions))
              self.history["valid_roc_auc"].append(self.score(X_val, y_val))

              if self.history["valid_roc_auc"][-1] < best_qual:
                abs_of_imp_count += 1

              else:
                abs_of_imp_count = 0
                best_qual = self.history["valid_roc_auc"][-1]

              if abs_of_imp_count == self.early_stopping_rounds:
                break

        if plot:
            self.plot_history()

    def predict_proba(self, X):
        total_predictions = np.zeros(X.shape[0])

        for i in range(len(self.models)):
          new_predicts = self.models[i].predict(X)
          gamma = self.gammas[i]
          total_predictions = total_predictions + gamma * self.learning_rate * new_predicts
          prob_1 = self.sigmoid(total_predictions)
          prob_0 = 1 - prob_1
          probs = np.column_stack((prob_0, prob_1))
        return probs

    def find_optimal_gamma(self, y, old_predictions, new_predictions) -> float:
        gammas = np.linspace(start=0, stop=1, num=100)
        losses = [self.loss_fn(y, old_predictions + gamma * new_predictions) for gamma in gammas]
        return gammas[np.argmin(losses)]

    def score(self, X, y):
        return score(self, X, y)

    def plot_history(self):
        """
        :param X: features array (any set)
        :param y: targets array (any set)
        """
        # подсчитывал в fit истории потерь или метрик для X_train, X_val (спрашивал у Danil Ivanov, он разрешил так делать), поэтому параметры X, y не нужны
        #if "train_loss" in self.history:
        x = range(len(self.history["train_loss"]))
        y = self.history["train_loss"]
        plt.plot(x, y, color='orange', linewidth=3)
        plt.xlabel('Итерация')
        plt.ylabel('Значения функции потерь')
        plt.title('График значения функции потерь от итерации на X_train')
        plt.show()

        if "valid_loss" in self.history:
          x = range(len(self.history["valid_loss"]))
          y = self.history["valid_loss"]
          plt.plot(x, y, color='orange', linewidth=3)
          plt.xlabel('Итерация')
          plt.ylabel('Значения функции потерь')
          plt.title('График значения функции потер от итерации на X_val')
          plt.show()

        #if "train_roc_auc" in self.history:
        x = range(len(self.history["train_roc_auc"]))
        y = self.history["train_roc_auc"]
        plt.plot(x, y, color='orange', linewidth=3)
        plt.xlabel('Итерация')
        plt.ylabel('Значения ROC-AUC')
        plt.title('График значения ROC-AUC от итерации на X_train')
        plt.show()

        if "valid_roc_auc" in self.history:
          x = range(len(self.history["valid_roc_auc"]))
          y = self.history["valid_roc_auc"]
          plt.plot(x, y, color='orange', linewidth=3)
          plt.xlabel('Итерация')
          plt.ylabel('Значения ROC-AUC')
          plt.title('График значения ROC-AUC от итерации на X_val')
          plt.show()